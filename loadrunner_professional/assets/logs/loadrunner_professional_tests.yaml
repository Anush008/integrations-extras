id: "loadrunner_professional"
tests:
  - sample: >+
        {"scenario_started":{"general":{"product": "LoadRunner Controller","version": "2024.1.0.0","path": "Scenario1","resultname": "","result_file": "C:\\Users\\user1\\AppData\\Local\\Temp\\2\\res\\res.lrr","start_time": 1707817084.000000,"daylight_bias": "0 minutes","time_zone": "0 seconds"},"scripts":[{"name": "webhttphtml1","type": "Multi+QTWeb","path": "C:\\Users\\user1\\Desktop\\WebHttpHtml1"}],"summary":{"scenario_type": "Manual Scenario","goal_profile_name": "Schedule 1","mode": "Scenario Scheduling","scenario_duration": "Start 25 Vusers: 5 every 00:00:05 (HH:MM:SS); Run for 00:01:00 (HH:MM:SS); Stop all Vusers: 5 every 00:00:05 (HH:MM:SS)","load_behavior": "Initialize each Vuser just before it runs"}}}
    result: null
  - sample: |+
        {"scenario_ended":{"execution":{"start_time": 1707817084.000000,"stop_time": 1707817214.000000,"total_duration": "130 seconds"},"general":{"product": "LoadRunner Controller","version": "2024.1.0.0","path": "Scenario1","resultname": "","result_file": "C:\\Users\\user1\\AppData\\Local\\Temp\\2\\res\\res.lrr","start_time": 1707817084.000000,"daylight_bias": "0 minutes","time_zone": "0 seconds"}}}
    result: null

# The `result` field should be left blank to start. Once you submit your log asset files with
# your integration pull-request in a Datadog GitHub repository, Datadog's validations will
# run your raw logs against your pipeline and return the result. If the result output in the
# validation is accurate, take the output and add it to the `result` field in your test YAML file.